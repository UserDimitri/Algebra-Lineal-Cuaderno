# -*- coding: utf-8 -*-
"""CUADERNO ALGEBRA LINEAL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d7jHb6f6Q33C3mLpji3cYIhbmUC4pgbS
"""



"""# **Parcial 1**

## ${Tema-1: Definición-y-tipos-de-matrices}$
---

${Definición}$

*Una matriz es un conjunto de números ordenados en filas y columnas.*

${Ejemplo-de-matriz}$

\begin{equation}
\begin{pmatrix}
2 & 5 & 0\\
7 & 3 & 8\\
3 & 0 & 1
\end{pmatrix}
\end{equation}

---

### **Crear una matriz en Python**

```python
A = np.array([[1,0,0],[3,4,0],[2,5,5]]) 
print(A)
```
---

${Tipos-de-matrices:}$

${Matriz-traspuesta:}$

*Una matriz donde las filas pasan a ser columnas y viceversa.*

${Ejemplo:}$

${A =}$

\begin{equation}
\begin{pmatrix}
3 & 2 & 4\\
8 & 9 & 0\\
9 & 1 & 5
\end{pmatrix}
\end{equation}

${A^T =}$

\begin{equation}
\begin{pmatrix}
3 & 8 & 9\\
2 & 9 & 1\\
4 & 0 & 5
\end{pmatrix}
\end{equation}


${Matriz-nula:}$

*Una matriz cuyos elementos son todos 0*

${Ejemplo:}$


\begin{equation}
\begin{pmatrix}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{pmatrix}
\end{equation}

${Matriz-identidad:}$

*Una matriz cuya diagonal principal está conformada por unos.*

${Ejemplo:}$

\begin{equation}
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}
\end{equation}

${Matriz-triangular-superior:}$

*Una matriz donde* ${a_ij}$ es igual a 0 y donde todas las filas son mayores que las columnas*

${Ejemplo:}$

\begin{equation}
\begin{pmatrix}
3 & 2 & 4\\
0 & 9 & 0\\
0 & 0 & 5
\end{pmatrix}
\end{equation}

${Matriz-triangular-inferior:}$

*Una matriz donde* ${a_ij}$ es igual a 0 y donde todas las filas son menores que las columnas*

${Ejemplo:}$

\begin{equation}
\begin{pmatrix}
2 & 0 & 0\\
4 & 1 & 0\\
3 & 6 & 9
\end{pmatrix}
\end{equation}

${Matriz-simétrica:}$

 *Una matriz es simétrica si es igual a su traspuesta*

 ${Ejemplo:}$

${A =}$

\begin{equation}
\begin{pmatrix}
1 & 2 \\
2 & 7 
\end{pmatrix}
\end{equation}

${A^T =}$

\begin{equation}
\begin{pmatrix}
1 & 2 \\
2 & 7 
\end{pmatrix}
\end{equation}

${Matriz-antsimétrica:}$

 *Una matriz es antisimétrica si su traspuesta es igual a su opuesta*

 ${Ejemplo:}$

${A =}$

\begin{equation}
\begin{pmatrix}
1 & 3 & 5\\
-3 & 1 & 6\\
-5 & -6 & 2
\end{pmatrix}
\end{equation}


${A^T =}$

\begin{equation}
\begin{pmatrix}
1 & -3 & -5\\
3 & 1 & -6\\
5 & 6 & 2
\end{pmatrix}
\end{equation}

*La diagonal principal no cambia de signo en la matriz antisimétrica*

---

## ${Tema-2: Operaciones-y-propiedades-con-matrices}$

### ${Operaciones-con-matrices}$

#### ${Suma:}$

**Importante:** *Las matrices que se vayan a operar deben tener el mismo orden, es decir, deben tener el mismo número de filas y de columnas.* 

${A_mn + B_mn = C_mn}$


*La suma de matrices consiste en sumar cada elemento de una matriz con su símil de la otra. ${Aij} + {Bij}$


${A=}$

\begin{equation}
\begin{pmatrix}
1 & 2 & 3\\
-1 & -2 & 0
\end{pmatrix}
\end{equation}

${B=}$

\begin{equation}
\begin{pmatrix}
0 & 1 & -3\\
3 & 2 & 1
\end{pmatrix}
\end{equation}

*Estas matrices tienen el mismo orden (2x3), por lo tanto, se pueden sumar*

*Si la matriz "A" fuera de orden 3x2, por ejemplo, no se podría.*

${Resultado}$

\begin{equation}
\begin{pmatrix}
1 & 3 & 0\\
2 & 0 & 1
\end{pmatrix}
\end{equation}

### **Suma de matrices en Python**

```python
m1 = np.array ([[1,2,3],[4,5,6],[7,8,9]]) #creando matriz 1
m2 = np.array ([[10,-3,5],[-5,3,7],[-11,4,6]]) #creando matriz 2
print(m1)
print(m2)
#Suma de matrices
m3 = m1 + m2
print(m3)
```

#### **Producto de una matriz por un escalar**

*Consiste en multiplicar un escalar cualquiera por todos y cada uno de los elementos de una matriz*

${Ejemplo:}$

3 * \begin{equation}
\begin{pmatrix}
1 & 3 & 0\\
2 & 0 & 1
\end{pmatrix}
\end{equation}

${Resultado}$

\begin{equation}
\begin{pmatrix}
3 & 9 & 0\\
6 & 0 & 3
\end{pmatrix}
\end{equation}

#### **Muliplicación de matrices**

*Consiste en multiplicar las filas de A por las columnas de B.*

${A*B = a_ij * b_jk = c_ik}$

**Importante:** *El número de columnas de la primera matriz debe ser igual al número de filas de la segunda matriz*

${Ejemplo}$

${A=}$

\begin{equation}
\begin{pmatrix}
2 & 1 & 5\\
1 & 3 & 2
\end{pmatrix}
\end{equation}

${B=}$

\begin{equation}
\begin{pmatrix}
3 & 4\\
-2 & 2\\
2 & 1
\end{pmatrix}
\end{equation}

${C=}$

\begin{equation}
\begin{pmatrix}
(2*3 + 1*-1+ 5*2) & (2*4 + 1*2 + 5*1)\\
(1*2 + 1*-1 + 2*2) & (1*4 + 3*2 + 2*1)
\end{pmatrix}
\end{equation}

${Resultado}$

\begin{equation}
\begin{pmatrix}
15 & 15\\
4 & 12
\end{pmatrix}
\end{equation}

### **Multiplicación de matrices**

```python
E = np.array ([[1,0,0],[0,1,0],[0,0,1]]) 
print(E)
A = np.array ([[1,2,-3],[0,1,2],[-1,2,0]])
print(A)
B = np.dot (E,A)
print(B)

```

---

### ${Operaciones-de-filas}$

*Se pueden hacer tres cosas:*

${1)}$ *Multiplicar un escalar por una fila*

${Ejemplo:}$

3 * [2 -1 7]

${2)}$ *Sumar o restar una fila con otra que puede estar multiplicada por un escalar o no*

${Ejemplo:}$

\begin{equation}
\begin{pmatrix}
1 & -3 & -5\\
3 & 1 & -6\\
5 & 6 & 2
\end{pmatrix}
\end{equation}

${F2 = F2 + 3 F3}$

**1) Se multiplica la fila 3 por 3**

(15 18 6)

**2) Se suma con la fila dos**

(3 1 -6) + (15 18 6) 

**3) Se obtiene la nueva fila 3**

(18 19 0) 



${3)}$ *Intercambiar filas*


${Ejemplo:}$

\begin{equation}
\begin{pmatrix}
1 & -3 & -5\\
3 & 1 & -6\\
5 & 6 & 2
\end{pmatrix}
\end{equation}

${F1 --> F2}$

\begin{equation}
\begin{pmatrix}
3 & 1 & 6\\
1 & -3 & -5\\
5 & 6 & 2
\end{pmatrix}
\end{equation}

---

### **Matriz reducida**

*Una matriz es reducida si:*

*a) El primer elemento distinto de 0 es 1*

*b) Cada columna de R que tiene este primer elemento 1 sus otros elementos son iguales a 0.*

**Nota:** *La matriz reducida es muy útil para resolver sistemas de ecuaciones*

${Ejemplo:}$

${A=}$

\begin{equation}
\begin{pmatrix}
0 & 1 & 0 & 5\\
1 & 0 & 0 & -1 \\
0 & 0 & 1 & 2
\end{pmatrix}
\end{equation}

${Reducida}$

\begin{equation}
\begin{pmatrix}
1 & 0 & 0 & -1\\
0 & 1 & 0 & -5 \\
0 & 0 & 1 & 2
\end{pmatrix}
\end{equation}

*En este caso, se intercambian las filas 1 y 2 para obtener el resultado. La idea es conseguir que la diagonal principal sea de unos y que la última columna tenga números distintos*

### **Matriz reducida en Python**

```python
EAR = sym.Matrix([[0,1,2,-1,1],[1,0,2,0,-1],[1,0,2,0,-1],[1,2,2,-2,3]])
EAR.rref(pivots=False)
```

---

### **Matriz inversa**

*Se expresa como:* ${A^{-1}}$

*Sea An una matriz cuadrada que opera sobre los reales; si existe
una matriz Bn cuadrada tal que AB = BA = I*

*A la matriz B se dice que es inversa de A y A es una matriz inversible; por
lo que:*

${B = A^{-1}}$

${Entonces:}$

${AA^{-1} = A^{-1}A = I}$

${Teorema:}$

*Si A es inversible entonces:* ${A^{-1}} = A $

${Teorema:}$

*Si ${A}$ y ${B}$ son inversibles entonces su producto es inversible y además
${(AB)}$ también es inversible y además:

${(AB)(AB) ^{-1}} = ABB^{-1}A^{-1}$

$ = {AIA^{-1}}$

$={AA^{-1}}$

$={I}$

*Si dos matrices son inversibles el producto es inversible*

### **Encontrar una matriz inversa**

1. Encontrar ${P(A)}$ -->  filas no nulas de ${A}$
2. Hallar ${R}$ escalonada.

 2.1 Si el rango de la reducida es igual a n ${P(R)=n}$

 2.2 La matriz es inversible ${P(R) = P(A)}$ por equivalencia.  


*1) Primero se agrega una matriz identidad a la derecha de una matriz cuadrada.*


\begin{equation}
\begin{pmatrix}
2 & -1 &/ 1 & 0\\
1 & 3 & / 0 & 1 
\end{pmatrix}
\end{equation}

*2) Haciendo operaciones de filas, se trasladará la matriz identidad al lado izquierdo.*

\begin{equation}
\begin{pmatrix}
1 & 0 &/ 3/7 & 1/7\\
0 & 1 & / -1/7 & 2/7 
\end{pmatrix}
\end{equation}

*3) Lo que quede en la lado derecho será la inversa de la matriz original.*

**Si la reducida es igual a una matriz identidad, la matriz es inversible.**

### **Matriz inversa en Python**

```python
A = np.array([[1,0,0],[3,4,0],[2,5,5]]) 
print(A)
inv_A = np.linalg.inv(A)
print(inv_A)
```

---

### **Definición de traza**

*Sea A una matriz de orden n.

${Tr(A)}$ = ${\sum_{i=1}^{n}}$ *(Suma de los elementos de la diagonal)*

### **Definición de Nilpotente**

*Es multiplicar una matriz por sí misma hasta que sea 0*

${A=}$

\begin{equation}
\begin{pmatrix}
1 & 1 \\
-1 & -1  
\end{pmatrix}
\end{equation}

${A^{2}=}$

${A * A =}$


\begin{equation}
\begin{pmatrix}
0 & 0 \\
0 & 0  
\end{pmatrix}
\end{equation}

### **Definición de idempotente**

*Es una matriz que da como resultado a sí misma al ser elevada a cualquier número*

### **Matriz ortogonal**

*Si ${A}$ * ${A^T}$ = ${D}$ (matriz diagonal)*

${Ejemplo:}$

${A=}$

\begin{equation}
\begin{pmatrix}
7 & 8 & 9\\
10 & 1 & 2\\
3 & 4 & 5
\end{pmatrix}
\end{equation}

${A^T=}$

\begin{equation}
\begin{pmatrix}
7 & 10 & 3\\
8 & 1 & 4\\
9 & 2 & 5
\end{pmatrix}
\end{equation}

${A*A^T=}$

\begin{equation}
\begin{pmatrix}
6 & 0 & 0\\
0 & 9 & 0\\
0 & 0 & 10
\end{pmatrix}
\end{equation}

### **Matriz ortonormal**

*Si ${A*A^T=I}$ (Matriz identidad)*

${Ejemplo:}$

${A=}$

\begin{equation}
\begin{pmatrix}
7 & 8 & 9\\
10 & 1 & 2\\
3 & 4 & 5
\end{pmatrix}
\end{equation}

${A^T=}$

\begin{equation}
\begin{pmatrix}
7 & 10 & 3\\
8 & 1 & 4\\
9 & 2 & 5
\end{pmatrix}
\end{equation}

${A*A^T=}$

\begin{equation}
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}
\end{equation}

---

# **Parcial 2**
---
### ***Definición***

*La determinante se define de la siguiente forma:*

${det A = |A|}$

### **Matrices 2x2**

${|A|= }$

\begin{vmatrix}
a_{11} & a_{12}\\ 
a_{21} & a_{22}\notag
\end{vmatrix}

${|A|= }$ ${a_11 * a_22 - a_21 * a_12}$

*Se resta la multiplicación de los elementos de la diagonal que va hacia abajo con la multiplicación de los elementos de la diagonal que va hacia arriba.*

### **Matrices 3x3**

${|A| = \sum_{j=1}^{n} (-1)^{i+j}*{a_{ij}}*|A_{ij}|}$

 
 $$ 
   = a_{11}\begin{vmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{vmatrix} 
   + a_{12}\begin{vmatrix} a_{23} & a_{21} \\ a_{33} & a_{31} \end{vmatrix} 
   + a_{13}\begin{vmatrix} a_{21} & a_{22} \\ a_{31} & a_{32} \end{vmatrix}
 $$

*1) Se toma un elemento de la matriz y se tacha la fila y la columna a la que pertenece*

*2) Con los elementos que no se han tachado se forma una determinante*

*3) Se calcula el determinante 2x2 y el resultado se multiplica con el elemento seleccionado previamente.*

**Nota:** *Si existe un elemento 0 dentro de una matriz, seleccionarlo ahorrará muchas operaciones*

**Signos**

\begin{vmatrix}
+ & - & +\\ 
- & + & -\\
+ & - & +\notag
\end{vmatrix}

---
### ***Propiedades***

*a) Si una fila es multiplo de otra fila, el valor del determinnante es igual a cero.*

*b)* |A*B| = |A| |B|

*c)* Si la matrices inversible entonces:

|A| ${\neq}$ 0 y ${|A^{-1}|}$ = $${\frac {{1}} {|A|}}$$

${AA^{-1}=A^{-1}A=I}$

${|AA^{-1}|=A^{-1}A=I=1}$

---

### ***Sistemas de ecuaciones de ${1^{er}}$ grado***

*Un sistema de ecuaciones se define de la siguiente forma:*


\begin{array}{rcl}
     a_{11}x_1+a_{12}x_2 +a_{13}x_3......a_{1n}x_n & = &b_1
  \\ a_{21}x_1+a_{22}x_2 +a_{23}x_3......a_{2n}x_n & = &b_2
  \\
  \\
  \\Hasta
  \\
  \\ a_{m1}x_1+a_{m2}x_2 +a_{m3}x_3......a_{mn}x_n & = &b_m
\end{array}

*Un sistema de ecuaciones de ecuaciones se puede respresentar como una matriz.*

 $$ 
   = \begin{bmatrix}
3 & 2 & 4\\
8 & 9 & 0\\
9 & 1 & 5
\end{bmatrix}
   * \begin{bmatrix}
x_1\\
x_2\\
x_n
\end{bmatrix}
   = \begin{bmatrix}
b_1\\
b_2\\
b_n
\end{bmatrix}
 $$

*b = son las respuestas de las ecuaciones*

${Teorema:}$

*Sea A ${\in}$ ${M_{mXn}}$ donde ${m<n}$ (más incógnitas que ecuaciones) entonces el sistema ${Ax^{T}=0}$ tiene al menos una solución.*


*Sea  ${m>n}$ (más ecuaciones que incógnitas) entonces el sistema ${Ax^{T}=b}$ tiene infinitas soluciones o ninguna solución.*


### ***Métodos para resolver sistemas de ecuaciones***

*1) Método de cramer*

\begin{array}{rcl}
     7x + 2x & = &10
  \\ 6x-y & = &13
\end{array}


*a) Se expresa el sistema de ecuaciones como un determinante*

\begin{vmatrix}
7 & 2\\ 
6 & -1\notag
\end{vmatrix}

*b) Resolvemos el determinante*

$$
=\begin{vmatrix}
7 & 2\\ 
6 & -1\notag
\end{vmatrix}
=
-7-12 =-19
$$

*C) Encontramos x con la siguiente fórmula:*

${x=\frac {{\Delta}x} {\Delta}}$

**Donde:**

${{\Delta}x}$ = *Determinante de x*

${{\Delta}}$ = *Determinante de la matriz*

**Resolución**

*1) Encontramos el determinante de x*

*1.1) Reemplazamos las respuestas del sistema de ecuaciones en la columna de x*

$$
\begin{vmatrix}
7 & 2\\ 
6 & -1\notag
\end{vmatrix}
->
\begin{vmatrix}
10 & 2\\ 
13 & -1\notag
\end{vmatrix}
=
-10-26 = -36
$$

*2) Reemplazamos en la fórmula:*

$$
{x=\frac {{\Delta}x} {\Delta}}
=
{\frac {-36} {-19} = \frac {36} {19} }
$$


*D) Encontramos y con la siguiente fórmula:*

${y=\frac {{\Delta}y} {\Delta}}$

**Donde:**

${{\Delta}y}$ = *Determinante de y*

${{\Delta}}$ = *Determinante de la matriz*

**Resolución**

*1) Encontramos el determinante de y*

*1.1) Reemplazamos las respuestas del sistema de ecuaciones en la columna de y*

$$
\begin{vmatrix}
7 & 2\\ 
6 & -1\notag
\end{vmatrix}
->
\begin{vmatrix}
7 & 10\\ 
6 & 13\notag
\end{vmatrix}
=
91-60 = 31
$$

*2) Reemplazamos en la fórmula:*

$$
{y=\frac {{\Delta}y} {\Delta}}
=
{\frac {31} {-19} = -\frac {36} {19} }
$$

*El método de cramer solo funciona con matrices cuadradas (que tiene el mismo múmero de filas y de columnas).*

### **Resolución de sistemas de ecuaciones con Python**

```python
A = np.array([[2,1,0],[0,-1,-1],[-2,1,1]])
B = np.array([1,2,-2])
X = np.linalg.solve(A, B)
print(X)
```

---

### **Matriz adjunta**

*Método para encontrar la inversa de una matriz*

${A^{-1}=\frac {Adj} {|A|}}$

\begin{bmatrix}
2 & 4 & 3\\
0 & 1 & -1\\
3 & 5 & 7
\end{bmatrix}

*1) Se obtiene la traspuesta:*

${A^{T}=}$

\begin{bmatrix}
2 & 0 & 3\\
4 & 1 & 5\\
3 & -1 & 7
\end{bmatrix}

*2) Se obtiene el determinante:*

$$ 
   = 2*\begin{vmatrix} 1 & -1\\ 5 & 7 \end{vmatrix} 
   + 0
   + 3*\begin{vmatrix} 4 & 3 \\ 1 & -1 \end{vmatrix}
$$

${|A|= 24-21 = 3}$

*3) Se obtiene la matriz adjunta:*

*3.1) Se tacha la fila y la columna de cada elemento y se obtiene su determinante.*

*4) Se reemplaza lo obtenido en la fórmula.*

---

###**Factorización LU**


*La matriz $A_{mxn}$ puede ser escrita como el producto de una matriz triangular ${L}$ y una matriz triangular superior ${U}$.*


${A = LU}$

**DONDE**

*L = Matriz triangular inferior*

\begin{bmatrix}
1 & 0 & 0\\
8 & 9 & 0\\
9 & 1 & 5
\end{bmatrix}

*U = Matriz triangular inferior*

\begin{bmatrix}
1 & 8 & 9\\
0 & 9 & 1\\
0 & 0 & 5
\end{bmatrix}

**Pasos para realizar una factorización LU:**

*1) Primero se reduce A a una matriz U mediante operaciones de fila.*
$$
U =\begin{bmatrix}
-2 & 0 & 0\\
0 & -3 & 1\\
10 & 12 & 3
\end{bmatrix}
$$
**F3 = f3 + 5F1**
$$
U =\begin{bmatrix}
-2 & 0 & 0\\
0 & -3 & 1\\
0 & 12 & 3
\end{bmatrix}
$$
**F3 = f3 + 4F2**
$$
U =\begin{bmatrix}
-2 & 0 & 0\\
0 & -3 & 1\\
0 & 0 & 7
\end{bmatrix}
$$
*2) L se forma a partir de los coeficientes utilizados en las operaciones realizadas para obtener U*

**F3 = f3 + 5F1** *En la fila 3 se usó un 5 para operar.*
$$
L = \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-5 & 0 & 1
\end{bmatrix}
$$
*Se reemplaza en el primer elemento de la fila 3 con signo opuesto (si fuera resta se mantiene el signo negativo).*
$$
L = \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-5 & -4 & 1
\end{bmatrix}
$$
**F3 = f3 + 4F2** *En la fila 3 se uso un 4 para operar.*


**Resolución sistemas de ecuaciones con factorización Lu**

\begin{array}{rcl}
     2x + y + 0 & = &1
  \\ 0-y-z & = &2
  \\ -2x+y+z & = &-2
\end{array}

*Expresamos el sistema de ecuaciones como matriz*

\begin{bmatrix}
2 & 1 & 0\\
0 & 1 & -1\\
-2 & 1 & 1
\end{bmatrix}

**F3 = F3 + 1F1**

\begin{bmatrix}
2 & 1 & 0\\
0 & 1 & -1\\
0 & 2 & 1
\end{bmatrix}

**F3 = F3 - 2F2**

$$
 U = \begin{bmatrix}
2 & 1 & 0\\
0 & 1 & -1\\
0 & 0 & 3
\end{bmatrix}
$$

*Reemplazamos los coeficientes de las operaciones en la matriz triangular inferior*

$$
 L = \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1 & 2 & 1
\end{bmatrix}
$$

**Usamos la fórmula:**

${L*y=b}$


$$
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1 & 2 & 1
\end{bmatrix}
*
\begin{bmatrix}
y_{1}\\
y_{2}\\
y_{3} 
\end{bmatrix}
=
\begin{bmatrix}
1\\
2\\
-2 
\end{bmatrix}
$$

*Calculamos y*

${y_{1}=1}$

${y_{2}=2}$

${-y_{1}+2y_{2}+y_{3}=-2}$

${-1 + 2(2) + y_{3}=-2}$

${-1+4+y_{3}=-2}$

${3+y_{3}=-2}$

${y_{3}=-2-3}$

${y_{3}=-5}$

*Usamos la fórmula:*

${U*(x,y,z) = y}$


$$
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1 & 2 & 1
\end{bmatrix}
*
\begin{bmatrix}
x\\
y\\
z 
\end{bmatrix}
=
\begin{bmatrix}
1\\
2\\
-5 
\end{bmatrix}
$$

${0+0+3z=-5}$

${z = \frac {-5} {3}}$

${0+y-z=2}$

---

${y - \frac {5} {3} = 2 }$

${y = 2 + \frac {5} {3}}$

${y = \frac {1} {3} }$

---

${2x + y +0 = 1}$

${2x + \frac {1} {3} + 0 = 1}$

${2x = 1-\frac {1} {3}}$

${2x = \frac {2} {3}}$

${x = \frac {2} {3}/2}$

${x = \frac {1} {3}}$

---

### **Factorización LU en Python**

```python
A = np.array ([[1,0],[-2,1]])
print(A)
U1 = np.array ([[1,0],[0,1]])
print(U1)
L1 = np.array ([[1,0],[-2,1]])
print(L1)
A1 = np.dot (L1,U1)
print(A1)
```

### **Vectores**

***Vector:*** Magnitud y sentido.

### **Operaciones entre vectores**

### **Suma y resta**


${V}$ = ${(v_{1}, v_{2})}$

${U}$ = ${(U_{1}, v_{2})}$

${V+U=}$ ${(v_{1}, v_{2})}$ + ${(u_{1}, u_{2})}$ = ${(v_{1} + u_{1}, v_{2} + u_{2})}$

${V-U=}$ ${(v_{1}, v_{2})}$ + ${(u_{1}, u_{2})}$ = ${(v_{1} - u_{1}, v_{2} - u_{2})}$

### **Norma de un vector**

${||U||=}$ ${\sqrt[2]{(y_{2}-y_{1})^{2}+(x_{2}-x_{1})^{2}}}$

${||U||=}$ ${\sqrt[2]{(x_{2})^{2}+(y_{1})^{2}}}$

//algo asi, nose

### **Producto escalar entre 2 vectores**

${U*V=}$ = ${||U||*||V||*cos \alpha}$

${cos\alpha=\frac{||V_{v}||} {||U||}}$

${||U_{u}||=\frac {u*v} {||V||}}$ *Norma de la proyección de ${U_{v}}$*

${||V_{V}||=\frac {u*v} {||U||}}$ *Norma de la proyección de ${V_{U}}$*

### **Producto escalar**

${U * V = U * V^{T}}$

${V}$ = ${(v_{1}, v_{2})}$ 

${U}$ = ${(U_{1}, v_{2})}$ 

$$
{U * V = \begin{pmatrix}
u_{1}\\
u_{2}
\end{pmatrix}}
*
{(v_{1}, v_{2})}
$$

${u*v=u_{1}*v_{1}+u_{2}*v_{2}}$

---

### **Espacios Vectoriales**
### **Definiciones**
### **Ley de composición interna***

${U*V=V'}$

${U,V}->(V, U) = U${\oplus}$V$

### **Ley de composición externa**
${K X V -> V}$

${\alpha * v -> \alpha {\displaystyle \odot } v }$

---

###**Propiedades**

### **Suma**

###**1) Clausurativa**

${\forall u, v \in R^{2}}$

${(u+v) \in R^{2}}$

$$
{(u+v) = (a_1,b_1)+(a_2,b_2)}
=
({a_1+a_2+b_1+b_2)}
$$

### **2) Conmutativa**

${\forall u, v \in R^{2}}$

$$
{(u+v)=(v+u)}
$$

$$
{(u+v) = (a_1+a_2,b_1+b_2}
=
(a_2+a_1,b_2+b_1)
$$

### **3) Asociativa**

${\forall u, v, w -> (u+v)+w=u+(v+w)}$

### **4) Existencia del neutro**

${\exists}$ ${\vec{0}}$ ${\in}$ ${R}$, ${\forall}$ ${v \in R^{2}}$

${\vec{V} + \vec{0} = \vec{V}}$

${\vec{0} + \vec{V} = \vec{V}}$

${\vec{0} = (0,0)}$

### **5) Existencia del inverso**

${\forall}$ ${v \in R^{2}}$ ${\exists}$ ${-v \in R^{2}}$

${\vec{V}+(\vec{-V}) = 0}$

${\vec{-V}+(\vec{V}) = 0}$

${\vec{V}+(\vec{-V}) = (a_1,b_1)+(-a_1,-b_1)}$

${\vec{V}+(\vec{-V}) = (a_1-a_2,b_1,-b_2)}$

${0,0 = \vec{0}}$

### ** 6) Clausurativa con respecto al producto**

${\forall \alpha \in R^{2}}$, ${\forall v \in R^{2}}$

${(\alpha * V) \in R^{2}}$

$$
{(\alpha * V)}
=
{\alpha(a_1,b_2)}
= {(\alpha a_2, \alpha b_2)} {\in R^{2}}
$$

### **7) Existencia del neutro escalar**

${\exists}$ 1 ${\in}$ ${R}$, ${\forall}$ ${v \in R^{2}}$

${1*\vec{V} = \vec{V}}$

${\vec{V}*1 = \vec{V}}$

### **8) Asociativa de escalares**

${\forall \alpha, \beta}$ ${\in}$ ${R}$

$$
{\forall V} {\in} {R^{2}}
->
(\alpha, \beta)* \vec {V}
=
\alpha(\beta * \vec {V})
\alpha\beta (a_1, b_1)
$$

$$
\alpha\beta (a_1, b_1) -> \alpha(\beta a_1, \beta b_1)
=
\alpha\beta a_1, \alpha\beta b_1)
=
\alpha\beta(a_1,b_1)
$$

### **9) Distributiva de escalares**

${\forall \alpha \beta {\in} {R}}$, ${\forall u, v {\in} {R}^{2}}$

${(\alpha + \beta)u = \alpha v +\beta u}$

${v(\alpha + \beta) = \alpha v +\beta v}$

### **10) Distributiva de vectores en suma**

${\forall \alpha {\in} {R}}$, ${\forall u, v {\in} {R}^{2}}$

${\alpha (u+v)= (\alpha u+\alpha v)}$

${(u+v) \alpha =  (u\alpha+v\alpha)}$

---

### **Otros espacios vectoriales**
${(R, R, +, \odot)}$ = *Espacio de los escalares*

${(R^{2}, R, +, \odot)}$ = *Espacio bidimiensional*

${(R^{3}, R, +, \odot)}$ = *Espacio tridimiensional*

### **Subespacios vectoriales**

*Dado un espacio vectorial ${V}$, decimos que ${W}$ es ${\neq}$ ${\emptyset}$ y ${w \subseteq V}$, es un subespacio vectorial vectorial de ${V}$ con respecto a las operaciones en ${V}$.*

### **Teorema**

*Dado e.v ${V}$ y dado un ${w \subseteq V}$, entonces ${w}$ es un subespacio vectorial de ${V}$ si:*

**1)** ${\vec {0} = w}$

**2)** ${\vec {x} \in w}$ y si ${\vec {y} \in w}$ *entonces ${\vec {x} + \vec {y} \in w}$*

**3)** ${\forall \alpha \in R}$ y si ${\vec {x} \in w}$ 

**Ejemplo: Determinar si ${w}$ es un s.e.v.**

${w=[x-y+z]}$

**1)** ${\vec {0}=w}$

${(0,0,0) + (x-y+z)=x-y+z}$

**2)** Si ${\vec {x} \in w}$ y si ${\vec {y} \in w}$ *entonces ${\vec {x} + \vec {y} \in w}$*

*Creamos dos vectores genéricos*

${\vec {x} = (x_1, x_2, x_3) \in w}$

${\vec {x} = (y_1, y_2, y_3) \in w}$

*Deben cumplir con la condición de w = ${(x,y,x) \in R /w=[x-y+z]}$*

${w=[x-y+z]}$ *condición*

**Sumamos**

${\vec {x} + \vec {y} = (x_1, x_2, x_3) + (y_1, y_2, y_3)}$

${(x_1+y_1),(x_2+y_2),(x_3 + y_3)}$

${x-y+z=0}$ *Condición*

${(x_1+y_1)-(x_2+y_2)+(x_3+y_3)=0}$

${x_1+y_1-x_2+y_2+x_3+y_3=0}$

*Agrupamos "x" con "x" y "y" con "y"*

${x_1-x_2+x_3)+(y_1-y_2+y_3)}$

*Sigue la misma estructura que ${x-y+z=0}$ y eso era igual a 0, por tanto*

${0+0=0}$

${0=0}$

**3)** si ${\forall \alpha \in R}$ y si ${\vec {x} \in w}$ 

${\alpha \in R}$

${\vec {x} \in w}$ = ${(x_1,x_2,x_3)}$

${\alpha\vec {x} = \alpha(x_1,x_2,x_3)}$

**Entonces:**

$$
{\alpha x_1 - \alpha x_2 + \alpha x_3}
=
{\alpha (x_1-x_2 +x_3)}
=
{\alpha(0)}
=
{0}
$$

${\alpha \vec {x} \in w}$

---

### **Combinaciones lineales**

*Sea ${(V, R, +, \odot)}$ y sea ${s = (v_1,v_2,V_3...V_n) \in V}$*

*Un vector ${v}$ que ${v\in V}$ se dice que es una combinación lineal de los vectores de ${S}$ si y solo si ${\alpha i, i =1,2...n}$ tal que:*

${U = \alpha v_1 + \alpha v_2 + \alpha v_3...\alpha_n v_n}$

**Ejemplo**

*Sea ${R^{2}}$ y sea ${v_1,v_2}$ donde ${v_1=(1,-1,4}$ y ${v_2=(2,-2,0)}$. Sea ${U=(0,0,8}$. Encuentre los coeficientes alfa para que ${U}$ sea una combinación lineal de ${S}$.

${U=\alpha v_1+\alpha v_2}$

${(0,0,8)=\alpha_1(1,-1,4) + \alpha_2(2,-2,0)}$


${(0,0,8)=(\alpha_1,-\alpha_1,4 \alpha_1) + (2\alpha_2,-2\alpha_2,0)}$

${(0,0,8)=(\alpha_1+2\alpha_2, -\alpha_2-2\alpha_2, 4\alpha_1)}$

$$
\begin{array}{rcl}
\alpha_1 + 2\alpha_2 & = &0
\\-\alpha_1 -2\alpha_2 & = &0
\\4\alpha_1 & = &8
\end{array}
$$

${\alpha_1 = 8/4 = 2}$

${\alpha_2 -2\alpha_2=0}$

${-2 -2\alpha_2=0}$

${-2\alpha_2=2}$

${\alpha_2=2/-2 = -1}$

**Solución** = ${V = 2(v_1) -v_2}$

---

### **Conjuntos linealmente independientes y dependientes**

**Definición**

*Sea ${s=(v_1,v_2...v_n)\in V}$

**a)** *Decimos que s es linealmente independiente si:*

${\alpha_1 v_1+\alpha_2 v_2...\alpha_n v_n = \vec {0}}$

**b)** *Decimos que es linealmente dependiente si:*

*Si en ${\alpha_1 v_1+\alpha_2 v_2...\alpha_n v_n = \vec {0}}$ (la combinación lineal) existe por lo menos un ${\alpha}$ que no es 0.*

**Ejemplo de resolución**

*Demostrar que los siguientes conjuntos son ${L.I}$ O ${L.D}$*

${s = [(1,2,3),(1,3,2),(0,1,-1)]}$

${s=[\alpha_1 (1,2,3)+\alpha_2(1,3,2)+\alpha_3 (0,1,-1) = (0,0,0)]}$

${(\alpha_1,2\alpha_1,3\alpha_1)+(\alpha_2,3\alpha_2,2\alpha_2)+(0,\alpha_3,-\alpha_3)=(0,0,0)}$

$$
\begin{array}{rcl}
\alpha_1 + \alpha_2 & = &0
\\2\alpha_1 +3\alpha_2 + \alpha_3 & = &0
\\3\alpha_1 + 2\alpha_2 -\alpha_3 & = &0
\end{array}
$$

**Representamos el sistema de ecuaciones como una matriz y hacemos la reducida**

\begin{bmatrix}
1 & 1 & 0 & | & 0\\
2 & 3 & 1 & | & 0\\
3 & 2 & -1 & | & 0
\end{bmatrix}


\begin{bmatrix}
1 & 0 & 0 & | & 0\\
0 & 1 & 1 & | & 0\\
0 & 0 & 0 & | & 0
\end{bmatrix}

$$
{\alpha_1 = 0}
$$

$$
{\alpha_2 = -\alpha_3}
$$

$$
0=0
$$



**Como hay una fila de ceros, el conjunto es linealmente dependiente.**

**Ejemplo 2:**

${s = [(1,0,1),(0,1,0),(0,-1,-1)]}$

${s=[\alpha_1 (1,0,1) +\alpha_2(0,1,0)+\alpha_3 (0,-1,-1) = (0,0,0)]}$

${(\alpha_1,0,-\alpha_1)+(0,\alpha_2,0)+(0,-\alpha_3,-\alpha_3)=(0,0,0)}$

$$
\begin{array}{rcl}
\alpha_1 + 0 +\alpha_3 & = &0
\\0 +\alpha_2 + 0& = &0
\\0 + -\alpha_2 -\alpha_3 & = &0
\end{array}
$$

**Representamos el sistema de ecuaciones como una matriz y hacemos la reducida**

\begin{bmatrix}
1 & 0 & 1 & | & 0\\
0 & 1 & 0 & | & 0\\
0 & -1 & -1 & | & 0
\end{bmatrix}


\begin{bmatrix}
1 & 0 & 0 & | & 0\\
0 & 1 & 0 & | & 0\\
0 & 0 & 1 & | & 0
\end{bmatrix}


$$
{\alpha_1 = 0}
$$

$$
{\alpha_2 = 0}
$$

$$
{\alpha_3 = 0}
$$

**Es linealmente independiente**

### **Independencia y dependencia en Python**

```python
#Los vectores linealmente independientes
vplantilla = np.array([])
v1 = np.array([1,2,3])
v2 = np.array([4,5,6])
v3 = np.array([7,8,9])
#El vector que queremos expresar como una combinación lineal de vectores independientes ([0,0,0])
target = np.array([0,0,0])
#Creamos una matriz independiente con los vectores independientes como columnas
matrix = np.column_stack ((v1, v2, v3))
#Resolviendo el sistema lineal para encontrar los coeficientes
coefficients = np.linalg.solve(matrix, target)
print("Coefficients", coefficients)
#Si da error no es linealmente independiente
```

---

### **Base**

**Definición:** *Sea ${B \subseteq V}$ , ${B \neq \emptyset}$

*Decimos que ${B}$ es una base de ${V}$ si hay dos condiciones:*

**1)** ${<B>=V}$ ${V = R^{2}}$

**2)** ${B=L.I}$ 

**Ejemplo:**

*Determinar si ${s=[(1,1),(-1,0)]}$ es base de R^{2}}*

${<B> = [(x,y)\in R^{2} / (x,y) = [\alpha_1(1,1) + \alpha_2 (-1,0)]]}$

${<B> = (\alpha_1,\alpha_1) + (\alpha_2, 0) = (x,y)}$

$$
\begin{array}{rcl}
\alpha_1 + \alpha_2 & = &x
\\\alpha_1 & = &y
\end{array}
$$

**Expresamos el sistema de ecuaciones como una matriz**

\begin{bmatrix}
1 & -1 & | & x\\
1 & 0 & | & y\\
\end{bmatrix}

**Realizamos la reducida:**

\begin{bmatrix}
1 & 0 & | & y\\
0 & 1 & | & y-x\\
\end{bmatrix}

$$
{\alpha_1 = y}
$$

$$
{\alpha_2 = y-x}
$$

**Reescribimos**

${<B>=[(x,y) \in R^{2}/(x,y) = y(1,1) + (y-x)(-1,0)]}$

*Los alphas generan ecuaciones.  ${s}$ podría ser base.*

**Comprobamos si es linealmente independiente**

\begin{bmatrix}
1 & 0 & | & 0\\
0 & 1 & | & 0\\
\end{bmatrix}

*Es linealmente independiente*

**Por lo tanto, ${s}$ es base de ${V}$**

---

### **Dimensión**

*Si el espacio vectorial ${V}$ tiene una base con un número finito de elementos, entonces, la dimensión de ${v}$ es el número de vectores en todas las bases y ${V}$ se denomina espacio vectorial de dimesnión finita, caso caso contrario es infinita.*

obs ${V}$ = {0} ${dimV = 0}$

**Teorema:** Si ${V}$ es s.e.v se dice que ${B}$ (base) es de dimensión finita y ${B}$ es una base de ${v_1}$ si tiene ${n}$ vectores finitos.

**Colorario**

*Si ${B}$ es s.e.v de dimensión finita, entonces, 2 bases cualquiera de ${V}$ deben tener el mismo número de vectores.*

**Ejemplo**

${R^{2},R,+,\odot}$

${R^{2}}$ La base debe tener 2 vectores


${Bc = [(1,0),(0,1)]}$

**1)***Si la dimensión ${v=n}$ entonces, cualquier base debe tener ${n}$ vectores.*

**Nota:** *Los polinomios tienen una dimensión mayor a la del subíndice, por cuanto, toda ${dim Po(x) = r}$ o por eje ${dim P_2(x) = 3}$.

**2)** *Sea ${v}$ s.e.v y ${dim \forall = m }$ etonces:

**a)** *Cualquier conjunto que contenga más de ${n}$ vectores es ${L.D}$ (linealmente dependiente).*

**b)** *Cualquier subconjunto de ${v}$ que contenga menos de ${n}$ vectores no puede generar el espacio ${v}$

**Ejemplo:**

*Encuentre una base y su dimensión para el espacio ${s}$ del sistema:*


\begin{array}{rcl}
\ 2x - y + 3z & = &0
\\4x -2y + 6z& = &0
\\-6x + 3y -9z & = &0
\end{array}


*Expresamos el sistema como matriz y realizamos la reducida*


\begin{bmatrix}
2 & -1 & 3 & | & 0\\
4 & -2 & 6 & | & 0\\
-6 & 3 & -9 & | & 0
\end{bmatrix}

=

\begin{bmatrix}
1 & -1/2 & 3/2 & | & 0\\
0 & 0 & 0 & | & 0\\
0 & 0 & 0 & | & 0
\end{bmatrix}

*Igualamos a (x,y,z)*

$$
\begin{bmatrix}
x\\
y\\
z
\end{bmatrix}
*
\begin{bmatrix}
1/2y-3/2z\\
y\\
z 
\end{bmatrix}
$$

*x es dependiente de dos variables, por lo tanto, la dimesnión es 2.*

---

### **Intersección de vectores**

Sea ${w_1}$, ${w_2}$ s.e.v de ${V}$ (dimensión finita) entonces, ${dim (w_1 \cap w_2) + dim(w_1+w_2)}$.

${dimw_1 +dimw_2=dim(w_1 \cap w_2) + dim(w_1 + w_2)}$

${dimw_1 + dimw_2 \geq (w_1+w_2)}$

${w = [w \in w/w=\alpha_1 v_1 + \alpha_2 v_2 +...+\alpha_n v_n]}$

**Base de ${w_1\cap w_2}$** = ${[v_1,v_2...v_k]}$

**Base de ${w_1}$** = ${[v_1,v_2...v_k,v_1...v_2...v_m]}$

**Base de ${w_2}$** = ${[v_1,v_2...v_k,w_1...w_2...w_n]}$

El s.e.v ${w_1+w_2}$ = ${[v_1,v_2...,v_k,v_1,v_2...v_m,w_1,w_2]}$

${(dimw_1 + dimw_2)=(k+m)+(k+m) = k+(k+m+n)}$

${K = (dim(w_1 \cap w_2)}$

${(k+m+n) = dim(w_1 + w_2)}$

**Encontrar la base de ${(w_1 \cap w2)}$**

Si:

${w_1=(x,y)\in R^{2}/x-y=0}$

${w_2=(x,y)\in R^{2}/x+2y=0}$

*Reescribir la base de ${w_1}$ de la siguiente manera (Para encontrar la dimensión de ${w_1}$)*

${(x,y) = x-y = 0, x = y}$

${w_1 = [(x,y)\in R^{2}/x=y [(y,y)\in R^{2}/y\in R]]}$

${(1,1) = dimw_1 = 1}$

*Reescribir la base de ${w_2}$ de la siguiente manera. Encontrar la dimensión de ${w_2}$

${(x,y)=x+2y =0, x=-2y}$

${w_2 = [(x,y)\in R^{2}/x=-2y [(-2y,y)\in R^{2}/y\in R]]}$

${(-2,1) = dimw_2 = 1}$

*Encontrar ${(w_1 \cap w_2)}$*

*${(w_1 \cap w_2) = [(x,y)\in R^{2} /x-y = 0 y x+2y=0]}$*

${[x-y = 0 y x+2y=0]}$* deben cumplirse estas 2 condiciones.

*Encontrar los "x" y los "y" que cumplen con esas dos condiciones*

\begin{array}{rcl}
\ x - y& = &0
\\-x +2y& = &0
\end{array}

${-3y=0}$

${y=0, x=0}$

*${(w_1 \cap w_2) = [(x,y)\in R^{2} /x = 0 y y=0]}$*

${(0,0) = R^{2}}$

${dim(w_1 \cap w_2) = 0}$

*No tiene base*

*Reemplazamos en la fórmula:*

${dimw_1 +dimw_2=dim(w_1 \cap w_2) + dim(w_1 + w_2)}$

${1+1=0+dim(w_1 + w_2)}$

${dim(w_1 + w_2)=2}$

---

### **Cambios de base**

*Sea ${v \cap V}$ y sea ${B_c=[v_1,v_2,v_3...V_n]}$ una base canónica de ${V}$.Entonces ${B' = [v_1,v_2,v_3...V_n]}$ otra base de ${v}$.

*Entonces:*

${v}_{B_c}$ = $[P]^{B'}_{B_c}$ $[u]_{B'}$

${v}_{B'}$ = $[P]^{B_c'}_{B'}$ $[v]_{B_C}$

**Ejemplo:**

${B_{Bc}=[(1,0),(0,1)]}$ (base canónica)

${B'=[(1,1),(1,0)]}$

${B_c=(3,2)}$ (Después de sumar todos los "x" y todos los "y")

Sacamos la inversa de ${B'}$

\begin{bmatrix}
1 & 1 & | & 1 & 0\\
1 & 0 & | & 0 & 1
\end{bmatrix}

= 

\begin{bmatrix}
1 & 0 & | & 0 & 1\\
0 & 1 & | & 1 & -1
\end{bmatrix}

*Multiplicamos la inversa por ${B_c}$*

$$
\begin{bmatrix}
0 & 1\\
1 & -1 
\end{bmatrix}
*
\begin{bmatrix}
3\\
2 
\end{bmatrix}
=
\begin{bmatrix}
a = 2\\
b = 1
\end{bmatrix}
$$

$$
u= \begin{bmatrix}
2\\
1
\end{bmatrix}
$$

### **Cambio de base en python**

```python
import numpy as np

#vector v en base canónica y base B' = Bp
v = np.array([[3,2]]) 
Bp = np.array([[1,1],[1,0]]) 

# u = P * v

#Encontramos la matriz de transformación P 
P = np.linalg.inv(Bp)

#Encontramos el vector v (que se llama u) eb la nueva base B'
u = np.dot (P, v)
print(u)

```

---

### **Vectores ortogonales y ortonormales**

${V = [R^n, R, +, \odot]}$

**Producto escalar:** ${<,> = \vec {v} * \vec {u} -> R}$

${<u,v> = \sum_ {i=1}^ {n} \alpha_i * \beta_i}$

${n=}$ Número de componentes de los vectores

*Donde ${u}$ es el vector compuesto por los ${\alpha}$ y ${v}$ es el compuesto por los ${\beta}$.

**Ejemplo:**

$$
u=\begin{bmatrix}
3\\
2
\end{bmatrix}
,
v=\begin{bmatrix}
1\\
5
\end{bmatrix}
$$


${<u,v> = \sum_ {i=1}^ {2} = \alpha_i * \beta_i}$

=

${\alpha_1* \beta_1+\alpha_2 * \beta_2}$

=

${3*1 + 2*5}$

=

${3+10}$

=

${13}$

### **Norma de un vector**

*Sea ${V = [R^n, R, +, \odot]}$ un espacio vectorial, se define la norma de un vector ${v \in V}$ como una función definida por:*

${||-|| : v-> R^+ u [0]}$

${||V||=  \sqrt [] {<v,v>}}$

### **Propiedades**

**1)** ${||v||\geq 0}$

**2)** ${||\alpha v= ||\alpha|| * ||v||. \alpha \in R}$

**3)** ${||u+v|| \leq ||u|| + ||v|| (desigualdad triangular)}$

**4)** ${||<u,v>|| \leq ||u||,||v||}$

**5)** ${||u-v||^2 = ||u||^2 + ||v||^2 - 2||u||*||v|| cos(\alpha)}$

**Definición:**

*Unconjunto de vectores en el espacio vectorial ${V}$ es llamado a ser ortogonal si cada par de vectores en el conjunto es ortogonal. El conjunto es llamado ortonormal si es ortogonal y cada vector es unitario*

**Ejemplo:**

*Demostrar: ${[(1,0,0),(0,\frac {3} {4},\frac {4} {5} ),(0,\frac {4} {5},-\frac {3} {5})]}$ es ortonormal.*

**1)** *Demostrar que que es ortogonal*

**Ortogonal:** *Cuando el producto escalar entre los elementos del conjunto es 0.*

$$
{<v_1,v_2>=[(1,0,0)*(0, \frac {3} {5}, \frac {4} {5})]}
= 
{(0,0,0)}
$$

$$
{<v_1,v_3>=[(1,0,0)*(0, \frac {4} {5}, -\frac {3} {5})]}
= 
{(0,0,0)}
$$

$$
{<v_2,v_3>=[(0, \frac {3} {5}, \frac {4} {5}) * (0, \frac {4} {5}, -\frac {3} {5})]}
= 
{(0,0,0)}
$$

**2)** *Calcular la norma de cada vector*

${||V_1||=  \sqrt [] {<v_1,v_1>}}$

$$
{<v_1,v_1= (1,0,0)*(1,0,0)>}
=
{1+0+0}
=
1
$$

${||V_2||=  \sqrt [] {<v_2,v_2>}}$

$$
{<v_2,v_2= (0, \frac {3} {5}, \frac {4} {5})*(0, \frac {3} {5}, \frac {4} {5})>}
=
{0+\frac {9} {15})+\frac {16} {5})}
=
1
$$

$$
{<v_3,v_3= (0, \frac {4} {5}, -\frac {3} {5})*(0, \frac {4} {5}, -\frac {3} {5})>}
=
1
$$

### **Vectores ortogonales y ortonormales en Python**

```python
#Obtenemos el producto escalar entre v1 y v2
v1 = np.array([2,-4]) 
v2 = np.array([2,1])
u = np.dot (v1, v2)
print(u)
#El resultado es 0, entonces si es ortogonal

#Obtenemos el producto escalar entre v1 y v1
v1 = np.array([2,-4]) 
v2 = np.array([2,-4])
u = np.dot (v1, v2)
print(u)
#El resultado es distinto de 1, entonces no es ortonormal.
#No es ortonormal

#Obtenemos los alphas
v1 = np.array([2,-4]) 
v2 = np.array([2,-4])

target = np.array([0,0])
matrix = np.column_stack ((v1, v2))

coefficients = np.linalg.solve(matrix, target)
print("Coefficients", coefficients)
#Da error, no es base.
```
---

### **Definición de conjunto ortogonal**

### **Proceso de ortogonalización de  Graham-Schmidt**

*Es convertir una base con subconjuntos ortogonales en bases ortogonales.*

**Teoría:** *Sea ${w}$ un s.e.v de producto interno ${v}$. Si ${B = [v_1,v_2,v_3...v_n]}$ es una base de ${V}$, entonces existe una base ${B_2=[w_1,w_2...w_n]}$ ortogonal.*

*Sea ${w_1=w_2}$:*

${w_2= \alpha_1 w_1 + \alpha_2 w_2}$

${<w_1,w_2>=0}$ =

${<w_1, \alpha_1 w_1 + \alpha_2 w_2, v_2>=0}$=

${<w_1, \alpha_1 w_1> + <\alpha_2 w_1, v_2>=0}$=

${\alpha_1<w_1, w_1> + <w_1, v_2>=0}$=

${\alpha_1 = \frac{<w_1,V_2>} {<w_1, w_2>}}$

**Fórmula de ${w_2}$**

${w_2 = \alpha_1 w_1 + \alpha_2 v_2}$

${w_2 = -\frac {<w_1, v_2>}{<w_1,w_2> w_1 + v_2}}$

${w_2 = v_2 - \frac {<w_1, v_2>}{w_1,w_1} w_1}$

*Para ${R^3}$*

${B_1 = [v_1, v_2, v_3]}$

${B_3 = [w_1, w_2, w_3]}$

**Fórmula de ${w_3}$**

*Sea ${v_1 = w_1}$, ${v_2 = w_2}$*

${w_3=\beta_1 v_1 + \beta_2 v_2 + \beta_3 v_3}$

${w_3=\beta_1 w_1 + \beta_2 w_2 + \beta_3 w_3}$ **1)**

${<w_1, w_3> = 0}$ y ${<w_2,w_3>=0}$; ${<w_1, w_2> = 0}$

${<w_1, w_3>=<w_1, \beta_1 w_1 + \beta_2 w_2 + \beta_3 w_3>}$

${\beta_1 <w_1,w_2> + \beta_2<w_1, w_2> + \beta_3 <w_1, w_2>}$

${\beta_1 <w_1,w_1> + \beta_3 <w_1, v_3> = 0}$

**Escojo** ${\beta_3 = 1}$ **2)**

${\beta_1 = - \frac {<w_1, v_3>} {w_1, w_2}}$ **3)**

**Encontramos \beta_2**

${<w_2,w_3>=0}$

${<w_2, \beta_1 w_1 + \beta_3 w_2 + \beta_2 w_3> = 0}$

${\beta_1 (0) + \beta_2 <w_2,w_1> + (1)v_3}$

${\beta_2 = \frac {-v_3} {<w_2, w_2>}}$ **4)**

**2)**, **3)** y **4)** en **1)**

${w_3 = v_3 - \frac {<w_1,v_3>} {<w_1, w_1>} w_1- \frac {<w_2,v_3>} {<w_2,w_2>}w_2}$

**Ejemplo:**

*Sea ${B =[v_1, v_2, v_3]}$*

${B =[\begin{bmatrix}
1\\
1\\
0
\end{bmatrix}, \begin{bmatrix}
0\\
1\\
1
\end{bmatrix}, \begin{bmatrix}
1\\
0\\
1
\end{bmatrix}]}$

*Hallar una base ortogonal ${B_3}$ tomando el cálculo B*

${B_3 = [w_1,w_2,w_3]}$

${w_1 = v_1}$

${w_1 = \begin{bmatrix}
1\\
1\\
0
\end{bmatrix}}$

${w_2 = v_2 - \frac {<w_1, v_2>}{w_1,w_1} w_1}$

${<w_1,v_2> = \begin{bmatrix}
1\\
1\\
0
\end{bmatrix} * [0,1,1] = 1}$

${<w_1,w_2> = \begin{bmatrix}
1\\
1\\
0
\end{bmatrix} * [1,1,0] = 2}$

${w_2 = v_2 - \frac {1} {2} w_1}$

${w_2 = \begin{bmatrix}
0\\
1\\
1
\end{bmatrix} - \frac {1} {2} \begin{bmatrix}
1\\
1\\
0
\end{bmatrix}}$

${w_2 \rightarrow \begin{bmatrix}
-\frac {1} {2}\\
\frac {1} {2}\\
1
\end{bmatrix} \rightarrow 2 * w_2 = \begin{bmatrix}
-1\\
1\\
2
\end{bmatrix}}$

${w_2 = \begin{bmatrix}
-1\\
1\\
2
\end{bmatrix}}$

${<w_1,w_2> = \begin{bmatrix}
1\\
1\\
0
\end{bmatrix} [-1,1,2]}$ =

${-1+1+0 = 0}$

# **Parcial 3**

### **Cónicas**

**Definición:** *El lugar geométrico de los puntos ${w}$ y ${A}$ relación de distancias a un punto y una recta fijos constante recibe el nombre de sección cónica o simplemente cónica*

**Punto fijo** ${\rightarrow}$ Foco

**Recta fija** ${\rightarrow}$ Directriz

**Relación constante** ${\rightarrow}$ Excentricidad ${(e)}$

---

**Categorías:**

Si ${e<1}$, la cónica se llama **ELIPSE**.

Si ${e = 1}$, la cónica se llama **PARÁBOLA**.

Si ${e > 1}$, la cónica se llama **HIPÉRBOLA**.

---

### **Parábola**

**Foco a la derecha de la directriz:** ${(y-k)^2=4a(x-h)}$
"""

import cv2
import matplotlib.pyplot as plt
path = "/content/parábola vertice izquierda.JPG"
img = cv2.imread(path)
plt.imshow(img)

"""**LL'** Recta

**${F}$** Puntos fijos

Se cumple que:

**${PF = PM}$**

**Directriz encima del foco:** ${(x-h)^2=-4a(y-k)}$
"""

import cv2
import matplotlib.pyplot as plt
path = "/content/Parabola concava hacia abajo.JPG"
img = cv2.imread(path)
im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(im_rgb)

"""**Foco a la izquierda de la directriz:** ${(y-k)^2 = -4a(x-h)}$"""

import cv2
import matplotlib.pyplot as plt
path = "/content/parabola vertice derecha.JPG"
img = cv2.imread(path)
im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(im_rgb)

"""**Foco encima de directriz:** ${(x-h)^2=4a(y-k)}$"""

import cv2
import matplotlib.pyplot as plt
path = "/content/Concava hacia arriba.JPG"
img = cv2.imread(path)
im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(im_rgb)

"""### **Elipse**"""

import cv2
import matplotlib.pyplot as plt
path = "/content/Elipse.JPG"
img = cv2.imread(path)
im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(im_rgb)

"""*Sean los puntos fijos ${F(-c,0)}$ y ${F(-c,0)}$ y ${2a}$ la suma constante ${(a>c)}$*

**Por definición:** ${F'P + PF = 2a}$

**Ecuación de la elpise:** ${\frac {x^2} {a^2} + \frac {y^2} {b^2}}$

*Si los focos fueran coordenadas ${F(0,c)}$ y ${(F(0,-c))}$, el* **EJE MAYOR** *estaría sobre el eje y, entonces la ecuación sería:*

**Ecuación focos en y:** ${\frac {x^2} {b^2} + \frac {y^2} {a^2}}$

**Excentricidad ${e}$:**

${e = \frac {c} {a} = \frac {\sqrt [] {a^2-b^3}} {a}}$

${c = ae}$

**Ecuaciones de las directrices ${D'D'}$ Y ${DD}$ respectivamente**

${x+\frac {a} {e} = 0}$ y ${x-\frac {a} {e} = 0}$ 

*Se denomina* **Lactus rectum** *de la elipse a la cuerda perpendicular al eje mayor por uno de los focos. Su longitu es ${\frac {2b^2} {a}}$*

*Si el centro de la elipse fuese el punto ${(h,k)}$ y el eje mayor tiene la dirección del eje x, la ecuación de la elipse sería igual a:*

${\frac {(x-h)^2} {a^2} + \frac {(y-h)^2} {b^2} = 1}$ eje mayor en el eje x

${\frac {(x-h)^2} {b^2} + \frac {(y-h)^2} {a^2} = 1}$ eje mayor en el eje y

**Forma general de la ecuación:**

${Ax^2+By^2+Dx + Ey + F = 0}$

### **Hipérbola**
"""

import cv2
import matplotlib.pyplot as plt
path = "/content/Hiperbola.JPG"
img = cv2.imread(path)
im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(im_rgb)

"""**Ecuación de la hipérbola:** 

${\frac {x^2} {a^2} - \frac {y^2} {b^2} = 1}$

${c^2 - a^2 = b^2}$

*Focos tienen coordenadas ${F(c,0)}$ y ${F'(-c,0)}$ *

*Distancia entre ${F y F'}$ es igual a:* ${2c}$

*Si los focos fueran ${F(0,c)}$ y ${F'(0,-c)}$ la ecuación sería:*

${\frac {y^2} {a^2} - \frac {x^2} {b^2} = 1}$

**Excentricidad ${e}$:**

${e = \frac {c} {a} = \frac {\sqrt [] {a^2-b^3}} {a}}$ , ${e>1}$

**Ecuaciones de las directrices DD':**

${X = \frac {+} {-} \frac {a} {e}}$. Cuando los focos están sobre el eje x.

${y = \frac {+} {-} \frac {a} {e}}$. Cuando los focos están sobre el eje y.

**Ecuaciones de las asíntotas:**

${y = \frac {+} {-} \frac {b} {a}x}$ ${\rightarrow}$ *Cuando el eje real es x*

${y = \frac {+} {-} \frac {a} {b}x}$ ${\rightarrow}$ *Cuando el eje real es y*

*Si el centro de la hipérbola es el punto de coordenadas ${(h,k)}$*

### **Eje real paralelo al eje x:** ${\frac {(x-h)^2} {a^2} - \frac {(y-k)^2} {b^2} = 1}$:
"""

import cv2
import matplotlib.pyplot as plt
path = "/content/hiperbola.png"
img = cv2.imread(path)
im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(im_rgb)

"""
### **Eje real paralelo al eje y:** ${\frac {(y-k)^2} {a^2} - \frac {(x-h)^2} {b^2} = 1}$: 
"""

import cv2
import matplotlib.pyplot as plt
path = "/content/hiperbola-eje-focal-vertical-origen.png"
img = cv2.imread(path)
im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(im_rgb)

"""# **Transformaciones ineales**

---

Sean ${V}$ y ${V'}$ dos espacios vectoriales sobre el campo ${K}$ y ${K}$ pasan a ser ${T: V \rightarrow W}$.

**Propiedades:**

**1)** ${T(\vec {u} + \vec {v}) = T(\vec {u}) + T(\vec {v})}$

**2)** ${T(c\vec {u})= cT(\vec {u})}$

**Ejemplo:**

*Demostrar la transformación lineal*

${T(v_1, v_2)=(v_1 - v_2, u_1 + 2v_2)}$

**1)** ${T(\vec {u} + \vec {v}) = T(\vec {u}) + T(\vec {v})}$
= 

${(\vec {u} + \vec {v})=(\vec {u_1},\vec {u_1}) + (\vec {v_1},\vec {v_1})}$ =

${(\vec {u_1} + \vec {v_1},\vec{u_2} + \vec {v_2})}$ =

${(\vec {u_1} + \vec {v_1}) = v_1}$ y ${(\vec {u_2} + \vec {v_2}) = v_2}$

${[(u_1 - u_2) + (u_2 + v_2), (u_1 + v_1) + 2(u_2 + v_2)]}$ =

${[(u_1- u_2) + (v_1-v_2),(u_1 + 2v_2) (u_1 + 2v_2) + (v_1 + 2v_2)]}$ =

${(u_1-u_2),(u_1+2v_2)+(v_1-v_2),(v_1+2v_2)}$ = ${T(\vec {u}) + T(\vec {v})}$


**2)** ${T(c\vec {u})= cT(\vec {u})}$ = ${cv_1 + cv_2}$

**Ejemplo 2:**

**Sea ${T: R^3 \rightarrow R^3}$ una transformación lineal tal que**

${T(1,0,0) = (2,-1,4)}$

${T(0,1,0) = (1,5,-2)}$

${T(0,0,1) = (0,3,1)}$

*Determine la transformación de ${T(2,3,-2)}$* 

${(2,3,-2) = 2(1,0,0)+3(0,1,0)-2(0,0,1)}$ =

**Reemplazamos en la siguiente fórmula:**

*${T(\vec {u}) = c_1T(v_1)+c_2T(v_2)+...+c_nT(v_n)}$*

${T(\vec {u}) = 2T(1,0,0)+3T(0,1,0)-2T(0,0,1)}$ =

${T(\vec {u}) = 2(2,-1,4)+3(1,5,-2)-2(0,3,1)}$ =

${(4,-2,8)+(3,15,-6)+(0,-6,-2)}$

${T(2,3,-2)=(7,7,0)}$

### **Código para demostrar si una transformación es lineal en Python**

```python
import numpy as np

def es_lineal(transformacion, dimension):
    # Generar vectores de entrada aleatorios
    v1 = np.random.rand(dimension)
    v2 = np.random.rand(dimension)

    # Verificar la propiedad de aditividad
    suma = transformacion(v1 + v2)
    suma_esperada = transformacion(v1) + transformacion(v2)
    if not np.allclose(suma, suma_esperada):
        return False

    # Verificar la propiedad de homogeneidad
    k = np.random.rand()
    producto = transformacion(k * v1)
    producto_esperado = k * transformacion(v1)
    if not np.allclose(producto, producto_esperado):
        return False

    # Si ambas propiedades se mantienen, se considera lineal
    return True

# Definir la transformación a probar
def transformacion(v):
    x = 2 * v[0]
    y = -3 * v[1]
    return np.array([x, y])

# Definir la dimensión de los vectores de entrada
dimension = 2

# Verificar si la transformación es lineal
if es_lineal(transformacion, dimension):
    print("La transformación es lineal")
else:
    print("La transformación no es lineal")
```

---

### **Transformaciones lineales dadas por matrices**

*Sea ${T:R^2 \rightarrow R^3}$, se define como:*

${T(\vec {v}) = A \cdotp \vec {v} = \vec {w}}$

**Dimensiones**

${A = (3x2)}$ ; ${\vec {v} = (2x1)}$; ${\vec {w} = (3x1)}$

${T: A \vec {v} \rightarrow \vec {w} }$

${T: R^n \rightarrow R^n}$

$$
\begin{bmatrix}
a_{11} & a_{12} & . & . & . & a_{1n}\\
a_{21} & a_{22} & . & . & . & a_{2n}\\
. & . & . & . & . & .\\
. & . & . & . & . & .\\
. & . & . & . & . & .\\
a_{m1} & a_{m2} & . & . &. & a_{mn}  
\end{bmatrix}
*
\begin{bmatrix}
v_1\\
v_2\\
. &\\
. &\\
. &\\
v_n  
\end{bmatrix}
=
\begin{bmatrix}
w_1\\
w_2\\
. &\\
. &\\
. &\\
w_n  
\end{bmatrix}
$$


**Ejemplo:**

*Demuestre que la ${T(\vec {v})}$ es una transformación lineal*

**a)** ${T(\vec {v}) = A \cdotp \vec {v} = \vec {w}}$ =


$$
\begin{bmatrix}
3 & 0\\
2 & 1\\
-1 & 2 
\end{bmatrix}
*
\begin{bmatrix}
-2\\
1 
\end{bmatrix}
=
\begin{bmatrix}
-6\\
-3\\
4 
\end{bmatrix}
$$

**b)** *${T(\vec {u} + \vec {v}) = T(\vec {u})+T(\vec{v})}$*

**2)** ${T(c \vec {u}) = cT(\vec {u})}$

*Sea ${\vec {u} \in R^2}$ y ${\vec {v} \in R^2}$*

**1)** ${T(\vec {u} + \vec {v}) = A(\vec {u} + \vec {v}) = A(\vec {u})+A(\vec {v})=T(\vec {u})+T(\vec {v})}$

**Ejemplo de resolución 2:**

*Encuentre la imagen y la preimagen de: * ${T(v_1, v_2)=(v_1+v_2,v_1-v_2)}$

${\vec {v} = (3,-4)}$

${\vec {w} = (3,19)}$

**Para encontrar la imagen reemplazamos los valores de ${v_1}$ en ${T(v_1, v_2)=(v_1+v_2,v_1-v_2)}$**

${T(v_1, v_2)=(3-4,3+4) = (-1,7)}$ ${\vec {w}}$ de ${\vec{v}}$

**Para encontrar la preimagen hacemos lo siguiente:**

${\vec {w} = (3,19)}$

*Según: ${T(v_1, v_2)=(v_1+v_2,v_1-v_2)}$*

${3 = v_1 + v_2}$

${19 = v_1-v_2}$

*Solucionamos el sistema de ecuaciones:*

${v_1=19+v_2}$

${19+v_2+v_2=3}$

${2v_2 = 3-19}$

${2v_2=-16}$

${v_2=-8}$

${v_1+v_2=3}$

${v_1-8=3}$

${v_1=3+8}$

${v_1=11}$

**La preimagen de ${\vec {w}=(3,19)}$ es ${\vec {v}=(11,-8)}$**

### **Código de transformaciones lineales dadas por matrices en Python**

```python
import numpy as np

# Definir la matriz de transformación
A = np.array([[2, 1], [-1, 3]])

# Definir el vector de entrada
v = np.array([1, 2])

# Realizar la transformación lineal
resultado = np.dot(A, v)

print("Resultado de la transformación lineal:", resultado)
```

---

### **Subespacios vectoriales de una transformación lineal**

**Definición:** *Sea ${T:V \rightarrow V'}$ una T.L* 

*Se define como núcleo de ${T}$, notado como ${N(T)}$:*

${N(T)=[v \in V / T(\vec {v}) = \vec {0}]}$

**Condiciones para demostrar que ${N(T)}$ es un s.e.v**

**1)** ${\vec {0} \in N(T)}$ 

${T(\vec {0}) = 0}$

${\vec {0} \in N(T)}$

*Esto quiere decir que el vector 0 pertenece al núcleo*

**2)** ${u, v \in V \rightarrow T(u) = \vec {0}}$ y ${T(v) = 0}$

${T(u) + T(v) = \vec {0} + \vec {0} = \vec {0}}$

${[T(u) + T(u)] \in N(T)}$

*Esto quiere decir que la suma de N transformaciones pertenece al núcleo*

**3)** ${\alpha T(v) \in N(T)}$

*Sea ${T(v) = \vec {0}}$*

${\alpha (T\vec{v}) = \alpha * 0 = \vec {0}}$

${\alpha T(\vec {v}) \in N(T)}$

*Esto quiere decir que si se multiploca una constante por la transformación de ${V}$, el resultado pertenece al núcleo de T*


*Se define como imagen de ${T}$, nota como ${Im(T)}$ a:*


${[w \in V' / \exists v \in V / T(v) = w]}$

**¿Cómo encontrar el núcleo de T y la imagen de T?**

**Encontrar el núcleo N(T):**

*Sea ${T:R^3 \rightarrow R^2}$* una T.L

$$
\begin{bmatrix}
x\\
y\\
z 
\end{bmatrix} {\rightarrow} T(\begin{bmatrix}
x\\
y\\
z 
\end{bmatrix}) = \begin{bmatrix}
x+y+z\\
y+z 
\end{bmatrix}
$$

**a)** ${N(T) = [v \in V / T(v) = \vec {0}]}$ ; ${v = R^3}$

$$
v = \begin{bmatrix}
x\\
y\\
z 
\end{bmatrix} ; T(\begin{bmatrix}
x\\
y\\
z 
\end{bmatrix}) = \begin{bmatrix}
x+y+z\\
y+z 
\end{bmatrix} = \begin{bmatrix}
0\\
0
\end{bmatrix}
$$

*Expresar como un sistema de ecuaciones*

$$
\begin{array}{rcl}
\ x + y + z & = &0
\\y+z & = &0
\end{array}
$$

*Expresar como una matriz y encontrar la reducida*

\begin{bmatrix}
1 & 1 & 1 & | & 0\\
0 & 1 & 1 & | & 0
\end{bmatrix} 

${F_1 \rightarrow F_1 - F_2}$

\begin{bmatrix}
1 & 0 & 0 & | & 0\\
0 & 1 & 1 & | & 0
\end{bmatrix} 

$$
\begin{array}{rcl}
\ x & = &0
\\y+z & = &0
\end{array} 
$$

*condiciones para que (x,y,z) de 0*

**Expresar las condiciones**

${N(T) = [(X,Y,Z) \in R^3 / x=0 ^ y=-z]}$

${[(0,-z,z) \in R^3 / z \in R]}$

${dim(N(T)) = 1}$

*Le damos un valor a z para generar una base*

${z = 1}$

**Base de ${N(T)}$** = ${[0,-1,1]}$

### **Encontrar la imagen de una transformación lineal en Python**

```python
import numpy as np

def encontrar_nucleo(transformacion):
    # Generar una matriz de prueba
    matriz_prueba = np.random.rand(3, 3)

    # Aplicar la transformación a la matriz de prueba
    resultado_transformacion = transformacion(matriz_prueba)

    # Encontrar el núcleo de la matriz de prueba
    nucleo = np.linalg.lstsq(matriz_prueba, resultado_transformacion, rcond=None)[0]

    return nucleo

# Definir la transformación lineal como una función
def transformacion(v):
    x = v[0] + v[1]
    y = 2 * v[1]
    z = v[0] - v[2]
    return np.array([x, y, z])

# Encontrar el núcleo de la transformación
nucleo = encontrar_nucleo(transformacion)

# Imprimir el núcleo
print("Núcleo de la transformación lineal:")
print(nucleo)
```

**b)** *Encontrar la imagen de ${T}$*

*Se hace exactamente lo mismo pero en lugar de igualar el sistema de ecuaciones a 0 se iguala a "a", "b"**

$$
v = \begin{bmatrix}
x\\
y\\
z 
\end{bmatrix} ; T(\begin{bmatrix}
x\\
y\\
z 
\end{bmatrix}) = \begin{bmatrix}
x+y+z\\
y+z 
\end{bmatrix} = \begin{bmatrix}
0\\
0
\end{bmatrix}
$$

*Expresar como un sistema de ecuaciones*

$$
\begin{array}{rcl}
\ x + y + z & = &0
\\y+z & = &0
\end{array}
$$

*Expresar como una matriz y encontrar la reducida*

\begin{bmatrix}
1 & 1 & 1 & | & a\\
0 & 1 & 1 & | & b
\end{bmatrix} 

${F_1 \rightarrow F_1 - F_2}$

\begin{bmatrix}
1 & 0 & 0 & | & a\\
0 & 1 & 1 & | & b
\end{bmatrix} 

$$
\begin{array}{rcl}
\ x & = &a-b
\\y+z & = &b
\end{array} 
$$

*Despejar "a" y reemplazo "b"*

${a=x+b}$

$$
\begin{array}{rcl}
\ a& = &x+b
\\b& = &y+z
\end{array}
$$

$$
v_1 = \begin{bmatrix}
x = 1\\
y = 0\\
z = 0
\end{bmatrix} v_2 = \begin{bmatrix}
x = 0\\
y = 1\\
z = 0
\end{bmatrix}
$$

**Imagen Im(T) =** ${[w \in V' = [(a,b) \in V']]}$

**Base de Im(T) =** ${(1,0), (1,1)}$

### **Encontrar la imagen de una transformación lineal en Python**

```python
import numpy as np

def encontrar_imagen(transformacion, vectores):
    # Aplicar la transformación a los vectores de entrada
    resultados = [transformacion(v) for v in vectores]

    # Convertir los resultados en una matriz
    imagen = np.array(resultados)

    return imagen

# Definir la transformación lineal como una función
def transformacion(v):
    x = 2 * v[0]
    y = -3 * v[1]
    return np.array([x, y])

# Definir los vectores de entrada
vectores = [np.array([1, 0]), np.array([0, 1]), np.array([-1, 1])]

# Encontrar la imagen de la transformación
imagen = encontrar_imagen(transformacion, vectores)

# Imprimir la imagen
print("Imagen de la transformación lineal:")
print(imagen)
```

---

### **Valores propios y vectores propios**

**Transformaciones lineales compuestas**


${T: V \rightarrow V'}$

${v \rightarrow T(v)}$

${\downarrow}$-------${\downarrow}$

${B}$-----${B'}$

**Fórmula para transformar una base a una matriz diagonal**

${B = P^{-1} A P}$

${D \rightarrow}$ *Matriz diagonal*

**¿Cómo se encuentra una matriz diagonal?**

${D = P^{-1} A P}$

**Teorema:**

**Def:** *Sea ${T:  V \rightarrow V' = v \rightarrow T(v) = Av}$*

*Es diagonizable si ${\exists B}$ base de ${v}$, ${B = [v_1, v_2, v_3;...,v_n]}$ tal que: ${Av = \lambda v}$.*

*Donde ${\lambda}$ es un escalar*

*${\lambda \rightarrow}$ se denomina como valor propio.*

*${v}$ se denomina como vector propio.*

${T(v_1)= \lambda_1 v_1+\lambda_2 v_2 + \lambda_3 v_3 +...+\lambda_n v_m = \lambda_1 v_1}$
${T(v_2)= \lambda_1 v_1+\lambda_2 v_2 + \lambda_3 v_3 +...+\lambda_n v_m=\lambda_2 v_2}$
${T(v_n)= \lambda_1 v_1+\lambda_2 v_2 + \lambda_3 v_3 +...+\lambda_n v_m = \lambda_n v_m}$

*Se concluye que c/valor propio está asociado a un vector propio.*

**Teorema:** ${T:  V \rightarrow V' = v \rightarrow T(v) = Av = \lambda v}$

*Si ${v}$ es un vector asociado a su vector propio ${\lambda}$ entonces:*

${|A - \lambda I| = 0}$

**Def:** A la determinante ${|A - \lambda I|}$ se llama ecuación característica o polinomio característico.

**Def:**  ${T:  V \rightarrow V' = v \rightarrow T(v) = Av = \lambda v}$ si se encuantra un conjunto ${B = [v_1, v_2, v_3;...,v_n]}$  L.I (Linealmente independiente) que califica como base, entonces ${T}$ es diagonizable.*

**Teormea:** *Si todos los valores propios de ${T:  V \rightarrow V'}$ son distintos, ${T}$ es diagonizable.

**Ejemplo:**

*Determianr si A es diagonizable, si lo es, encontrar ${DP}$*

$$
A = \begin{bmatrix}
1 & -1 & 4\\
3 & 2 & -1\\
2 & 1 & -1
\end{bmatrix}
$$

**Primer paso:** Encontrar los ${\lambda}$ mediante ${|A-\lambda I| = 0}$

$$
\begin{bmatrix}
1 & -1 & 4\\
3 & 2 & -1\\
2 & 1 & -1
\end{bmatrix}
-
\lambda \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & -1 & 4\\
3 & 2 & -1\\
2 & 1 & -1
\end{bmatrix}
-
\begin{bmatrix}
\lambda & 0 & 0\\
0 & \lambda & 0\\
0 & 0 & \lambda
\end{bmatrix}
=
\begin{bmatrix}
1-\lambda & -1 & 4\\
3 & 2-\lambda & -1\\
2 & 1 & -1-\lambda
\end{bmatrix}
$$

**Paso 2:** *Encontrar el determinante*

$$ 
   = (-1)^2 (1-\lambda)\begin{vmatrix} 2-\lambda & -1 \\ 1 & -1-\lambda \end{vmatrix} 
   + (-1)^3 (-1)\begin{vmatrix} 3 & -1 \\ 2 & -1-\lambda \end{vmatrix} 
   +(-1)^4 (4)\begin{vmatrix} 3 & 2-\lambda \\ 2 & 1 \end{vmatrix}
 $$

 *Al resolver el determinante queda esto:*

 ${\lambda^3 - 2\lambda ^2 - 5\lambda + 6 = 0}$

 ${\lambda_1 = 1}$

 ${\lambda_2 = -2}$

 ${\lambda_3 = 3}$

 **Reemplazamos en la matriz diagonal**

 $$
D = \begin{bmatrix}
1 & 0 & 0\\
0 & -2 & 0\\
0 & 0 & -3
\end{bmatrix}
$$

**Paso 3:** *Encontrar los vectores propios*

${|A-\lambda I| = \vec {0}}$

${\lambda_1 = 1}$

$$
\begin{bmatrix}
0 & -1 & 4\\
3 & 1 & -1\\
2 & 1 & -2
\end{bmatrix} v_1
=
\begin{bmatrix}
0\\
0\\
0
\end{bmatrix} 
$$

\begin{bmatrix}
0 & -1 & 4 & | & 0\\
3 & 1 & -1 & | & 0\\
2 & 1 & -2 & | & 0
\end{bmatrix} 

**Realizamos la reducida**


\begin{bmatrix}
1 & 0 & 1 & | & 0\\
0 & 1 & -4 & | & 0\\
0 & 0 & 0 & | & 0
\end{bmatrix} 

${x_1 + z_1 = 0 \rightarrow x_1 = -z_1}$

${y_1 - 4z_1 = 0 \rightarrow y_1 = 4z_1}$

${v_1 = [(-z_1, 4z_1, z_1)/ z \in R]}$

${v_1= (-1,4,1)}$

*Se hace exactamente lo mismo con los demás ${\lambda}$ y el resultado es:*

${\lambda_2 = -2}$

*Reducida*

\begin{bmatrix}
1 & 0 & 1 & | & 0\\
0 & 1 & -1 & | & 0\\
0 & 0 & 0 & | & 0
\end{bmatrix} 

${x_2 + z_2 = 0 \rightarrow x_2 = -z_2}$

${y_2 + z_2 = 0 \rightarrow y_2 = -z_2}$

$$
\begin{bmatrix}
-z_2\\
z_2\\
z_2
\end{bmatrix} 
=
v_2 = \begin{bmatrix}
-1\\
1\\
1
\end{bmatrix} 
$$

${\lambda_3 = 3}$

*Reducida*

\begin{bmatrix}
1 & 0 & -1 & | & 0\\
0 & 1 & -2 & | & 0\\
0 & 0 & 0 & | & 0
\end{bmatrix} 

${x_2 - z_2 = 0 \rightarrow x_2 = z_2}$

${y_2 + z_2 = 0 \rightarrow y_2 = 2z_2}$

$$
\begin{bmatrix}
z_3\\
2z_3\\
z_3
\end{bmatrix} 
=
v_2 = \begin{bmatrix}
1\\
2\\
1
\end{bmatrix} 
$$

**Usamos los vectores propios obtenidos para crear la matriz ${P}$**

$$
P = \begin{bmatrix}
-1 & -1 & 1\\
4 & 1 & 2\\
1 & 1 & 1
\end{bmatrix}
$$

**Encontramos la inversa de ${P}$**


$$
P^{-1} = \begin{bmatrix}
-1 & -1 & 1 & | & 1 & 0 & 0\\
4 & 1 & 2 & | & 0 & 1 & 0\\
1 & 1 & 1 & | & 0 & 0 & 1
\end{bmatrix}
=
\begin{bmatrix}
-\frac {1} {6} & \frac {1} {3}  & -\frac {1} {2} \\
-\frac {1} {3}  & -\frac {1} {3}  & 1\\
\frac {1} {2}  & 0 & \frac {1} {2} 
\end{bmatrix}
$$

**Diagonalizamos usando ${D = P^{-1} A P}$**

$$
D= \begin{bmatrix}
-\frac {1} {6} & \frac {1} {3}  & -\frac {1} {2} \\
-\frac {1} {3}  & -\frac {1} {3}  & 1\\
\frac {1} {2}  & 0 & \frac {1} {2} 
\end{bmatrix}
*
\begin{bmatrix}
1 & -1 & 4\\
3 & 2 & -1\\
2 & 1 & -1
\end{bmatrix}
*
\begin{bmatrix}
-1 & -1 & 1\\
4 & 1 & 2\\
1 & 1 & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0\\
0 & -2 & 0\\
0 & 0 & 3
\end{bmatrix}
$$

### **Encontrar los valores y los vectores propios en python**

```python
import numpy as np

def encontrar_vectores_propios(transformacion):
    # Obtener los valores propios y vectores propios de la matriz de la transformación
    valores_propios, vectores_propios = np.linalg.eig(transformacion)
    
    return valores_propios, vectores_propios

# Ejemplo de uso
transformacion_ejemplo = np.array([[2, 1], [1, 3]])
valores_propios, vectores_propios = encontrar_vectores_propios(transformacion_ejemplo)

print("Valores propios:")
for valor_propio in valores_propios:
    print(valor_propio)

print("Vectores propios:")
for vector_propio in vectores_propios.T:
    print(vector_propio)
```
"""